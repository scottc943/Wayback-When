import requests
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
from urllib.parse import urljoin, urlparse
import waybackpy
from datetime import datetime, timedelta, timezone
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import concurrent.futures
from collections import deque
import threading
import warnings
from IPython.display import clear_output # Import clear_output for console management

# Import selenium components
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService # Import ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By # Import By for CAPTCHA detection

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# Install required Python packages
!pip install requests beautifulsoup4 waybackpy selenium webdriver-manager

# Install google-chrome-stable for better compatibility with ChromeDriver
!wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -
!echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | tee /etc/apt/sources.list.d/google-chrome.list
!apt-get update && apt-get install -y google-chrome-stable

# Function to set up and return a headless Chrome WebDriver
def get_driver():
    options = Options()
    options.add_argument("--headless") # Run Chrome in headless mode
    options.add_argument("--no-sandbox") # Bypass OS security model, required for Colab
    options.add_argument("--disable-dev-shm-usage") # Overcome limited resource problems
    options.add_argument("--disable-gpu") # Added for headless stability
    # Explicitly set the binary location for google-chrome-stable
    options.binary_location = '/usr/bin/google-chrome'
    # Optionally, add a user-agent to mimic a regular browser
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    # Initialize ChromeDriver using webdriver_manager to handle downloads and setup
    service = ChromeService(executable_path=ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver


# Define Settings Dictionary
SETTINGS = {
    'archiving_cooldown': 2, # Default cooldown in days
    'urls_per_minute_limit': 15, # Max URLs to archive per minute
    'max_crawler_workers': 30 # Max concurrent workers for website crawling
}

# Configure a retry strategy once, to be used for each new session
retry_strategy = Retry(
    total=5,
    backoff_factor=1,
    status_forcelist=[403, 404, 429, 500, 502, 503, 504],
    allowed_methods=False # Apply retry to all HTTP methods
)
adapter = HTTPAdapter(max_retries=retry_strategy)

# Global dictionary to store the last access time for each domain
# Removed: last_domain_access_time = {}
# Lock to manage concurrent access to last_domain_access_time
# Removed: domain_cooldown_lock = threading.Lock()

def get_internal_links(base_url, driver): # Modified to accept a driver object
    """Scrapes a given URL to find all internal links and returns them as a set.

    This function uses a browser-like User-Agent and retry logic for robustness.
    Contributors: Consider enhancing error handling for different HTTP status codes or improving URL normalization.
    """
    links = set()

    # Extract the domain from the base_url
    parsed_base_url = urlparse(base_url)
    domain = parsed_base_url.netloc

    # Removed: Domain-specific cooldown logic

    # The headers dictionary is largely for requests.Session, not directly for Selenium's driver.get()
    # User-Agent is set in get_driver options.
    # Other headers are handled by the browser context Selenium provides.

    try:
        # Navigate to the base_url using Selenium
        driver.get(base_url)

        # CAPTCHA Detection
        captcha_indicators = [
            (By.ID, 'g-recaptcha'),
            (By.CLASS_NAME, 'g-recaptcha'),
            (By.XPATH, "//iframe[contains(@src, 'google.com/recaptcha')]")
        ]
        captcha_detected = False
        for by_type, value in captcha_indicators:
            if driver.find_elements(by_type, value):
                captcha_detected = True
                break

        if captcha_detected:
            while True:
                print(f"[CAPTCHA DETECTED] for {base_url}.\nPlease solve the CAPTCHA manually in the browser if it becomes visible.\nType 'continue' to resume crawling or 'skip' to skip this URL:")
                user_choice = input().strip().lower()
                if user_choice == 'continue':
                    print("Attempting to continue after manual intervention...")
                    break # Break the loop to re-attempt processing the page
                elif user_choice == 'skip':
                    print(f"Skipping {base_url} due to CAPTCHA.")
                    return links # Return empty set if CAPTCHA detected and skipped
                else:
                    print("Invalid input. Please type 'continue' or 'skip'.")

        # Parse the HTML content of the page using BeautifulSoup from driver.page_source
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        # Extract the domain name from the base_url to identify internal links
        domain = parsed_base_url.netloc

        # Find all anchor tags (<a>) that have an 'href' attribute
        for anchor in soup.find_all('a', href=True):
            href = anchor['href']

            # Skip links that are fragments, mailto links, javascript, or contain 'action='
            # These are typically not relevant for archiving content.
            if href.startswith(('#', 'mailto:', 'javascript:')) or 'action=' in href:
                continue

            # Resolve relative URLs to absolute URLs
            full_url = urljoin(base_url, href)
            parsed_url = urlparse(full_url)

            # Check if the parsed URL's domain matches the base URL's domain
            # This ensures only internal links are collected.
            if parsed_url.netloc == domain:
                # Construct a clean URL without query parameters or fragments
                clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
                links.add(clean_url)

    # Handle specific HTTP errors during the request (Selenium errors are different from requests)
    except Exception as e:
        print(f"An error occurred while crawling {base_url} with Selenium: {e}")

    return links

def should_archive(url, global_archive_action):
    """Determines if a URL should be archived based on a global action or a custom check.

    Contributors: Consider adding more sophisticated checks for archiving, e.g., checking content changes.
    """
    user_agent = "ArchiveRequestBot/1.0" # Define a user agent for Wayback Machine requests
    wayback = waybackpy.Url(url, user_agent) # Initialize WaybackPy URL object

    # If the global action is 'a' (Archive All) or 's' (Skip All), we just return the boolean
    # and let the calling function handle consolidated printing.
    if global_archive_action == 'a':
        return True, wayback
    elif global_archive_action == 's':
        return False, wayback

    # If global_archive_action is 'n' (Normal), proceed with the 48-hour check logic.
    while True:
        try:
            # Get the most recent archive record for the URL from Wayback Machine
            newest = wayback.newest()
            # Extract the timestamp of the last archive and make it timezone-aware (UTC)
            # WaybackPy's timestamp is naive but represents UTC, so we make it explicit.
            last_archived_dt = newest.timestamp.replace(tzinfo=timezone.utc)

            # Get the current UTC time for comparison
            current_utc_dt = datetime.now(timezone.utc)
            # Calculate the time difference since the last archive
            time_diff = current_utc_dt - last_archived_dt

            # If the last archive was less than `archiving_cooldown` days ago, skip archiving
            if time_diff < timedelta(days=SETTINGS['archiving_cooldown']):
                print(f"[-] Skipping: {url} (Last archived {time_diff.total_seconds() // 3600:.1f} hours ago)")
                return False, wayback
            # Otherwise, the URL needs archiving
            else:
                print(f"[+] Needs Archive: {url} (Last archived {time_diff.total_seconds() // 3600:.1f} hours ago, > {SETTINGS['archiving_cooldown']*24} hours)")
                return True, wayback

        # Handle cases where no existing archive record is found for the URL
        except waybackpy.exceptions.NoCDXRecordFound:
            print(f"[!] No existing archive found for {url}. Archiving.")
            return True, wayback
        # Handle other unexpected errors during the archive check
        except Exception as e:
            print(f"[!] An error occurred while checking archive for {url}: {e}")
            # Prompt the user to retry or skip the URL in case of an error
            user_choice = input("Type 'retry' to try again or 'skip' to skip this URL: ").strip().lower()
            if user_choice == 'retry':
                print("Retrying in 5 seconds...")
                time.sleep(5) # Wait before retrying the archive check
            elif user_choice == 'skip':
                print(f"Skipping {url} due to user request.")
                return False, wayback # Skip this URL if user chooses to
            else:
                print("Invalid input. Please type 'retry' or 'skip'.")

# A lock to ensure only one thread modifies `last_archive_time` at a time
archive_lock = threading.Lock()
# Timestamp of the last archive request, initialized to 0.0
last_archive_time = 0.0
# Minimum delay required between archive requests to respect rate limits
MIN_ARCHIVE_DELAY_SECONDS = 60 / SETTINGS['urls_per_minute_limit']

def process_link_for_archiving(link, global_archive_action):
    """Checks if a link needs archiving and attempts to save it to Wayback Machine with rate limiting and retries.

    Contributors: Optimizing rate limiting or adding more detailed logging for archive results would be valuable.
    """
    global last_archive_time

    # Determine if the link needs to be saved based on global action or 48-hour rule
    needs_save, wb_obj = should_archive(link, global_archive_action)

    if needs_save:
        retries = 3 # Number of retries for archiving a single link
        while True:
            # Acquire a lock to safely manage the global rate limit timer
            with archive_lock:
                now = time.time() # Current time
                elapsed = now - last_archive_time # Time since the last archive request
                # If the elapsed time is less than the minimum required delay, pause.
                if elapsed < MIN_ARCHIVE_DELAY_SECONDS:
                    sleep_duration = MIN_ARCHIVE_DELAY_SECONDS - elapsed
                    print(f"[RATE LIMIT] Sleeping for {sleep_duration:.2f} seconds before archiving {link}")
                    time.sleep(sleep_duration)

                # Update the last archive time after potentially sleeping
                last_archive_time = time.time()

            try:
                print(f"[+] Archiving: {link}...")
                wb_obj.save() # Attempt to save the URL to Wayback Machine
                return f"Successfully archived: {link}"
            except Exception as e:
                error_message = str(e)
                # Check for a specific rate limit error message from Wayback Machine
                rate_limit_keyword = 'Save request refused by the server. Save Page Now limits saving 15 URLs per minutes.'

                if rate_limit_keyword in error_message:
                    # If rate limit hit, prompt user for action (retry or skip)
                    while True:
                        user_choice = input(f"Wayback Machine rate limit hit for {link}. Type 'R' to retry after 5 minutes or 'S' to skip this URL: ").strip().lower()
                        if user_choice == 'r':
                            print("Pausing for 5 minutes as requested...")
                            time.sleep(300) # Pause for 5 minutes (300 seconds)
                            break # Break from this inner loop to re-attempt archiving
                        elif user_choice == 's':
                            print(f"Skipping {link} as requested.")
                            return f"User skipped archiving due to rate limit: {link}"
                        else:
                            print("Invalid input. Please type 'R' or 'S'.")
                else:
                    # For other errors, decrement retry count and retry if attempts remain
                    retries -= 1
                    if retries > 0:
                        print(f"[!] Could not save {link}: {e}. Retrying ({retries} attempts left)...")
                        time.sleep(2) # Short cooldown before next retry for other errors
                    else:
                        # If no retries left, report failure
                        return f"[!] Failed to archive {link} after multiple attempts: {e}"
    else:
        return f"Skipped: {link}"

def crawl_website(base_url):
    """
    Performs a breadth-first search (BFS) to discover all internal links within a given base URL.
    Uses parallel processing for efficient scraping.

    Contributors: Optimizing the BFS algorithm (e.g., using a faster data structure for visited URLs),
    or improving the parallel processing strategy could be future enhancements.
    """
    # Removed driver initialization from here. Each worker will get its own driver.

    # Initialize a deque (double-ended queue) for BFS, starting with the base_url
    queue = deque([base_url])
    # Keep track of visited URLs to avoid redundant processing and infinite loops
    visited_urls = {base_url}
    # Stores all unique internal links discovered during the crawl
    all_unique_internal_links = {base_url}

    # Wrapper function to create and quit a driver for each task
    def wrapper_get_internal_links(url_to_crawl):
        driver = None
        try:
            driver = get_driver()
            links = get_internal_links(url_to_crawl, driver)
            return links
        finally:
            if driver:
                driver.quit()

    try:
        # Use ThreadPoolExecutor for concurrent processing of `get_internal_links` calls
        with concurrent.futures.ThreadPoolExecutor(max_workers=SETTINGS['max_crawler_workers']) as executor:
            while queue:
                # Define a batch size for processing URLs in parallel
                batch_size = SETTINGS['max_crawler_workers'] # Use max_crawler_workers as batch size
                current_batch_urls = [] # Stores just URLs, no longer (url, driver) tuples

                # Populate the batch with URLs from the queue, up to `batch_size`
                while queue and len(current_batch_urls) < batch_size:
                    url_to_crawl = queue.popleft() # Get the next URL from the front of the queue
                    current_batch_urls.append(url_to_crawl) # Append just the URL

                # If the current batch is empty, there are no more URLs to process
                if not current_batch_urls:
                    break

                print(f"Processing batch of {len(current_batch_urls)} URLs for crawling.")

                # Submit `wrapper_get_internal_links` for each URL in the batch to the executor.
                for new_links_from_url in executor.map(wrapper_get_internal_links, current_batch_urls):
                    for link in new_links_from_url:
                        all_unique_internal_links.add(link) # Add discovered link to the overall set
                        # Print discovered URL if it's new
                        if link not in visited_urls:
                            print(f"[DISCOVERED] {link}")
                            visited_urls.add(link)
                            queue.append(link)
    except Exception as e:
        print(f"An error occurred during website crawling: {e}")
    finally:
        # No driver.quit() here as drivers are managed per thread in wrapper_get_internal_links
        pass

    return all_unique_internal_links

def main():
    """Main function to orchestrate the website crawling and archiving process.

    Contributors: Consider adding command-line argument parsing for URLs, or integrating a configuration file.
    """
    # Removed: Clear the last_domain_access_time dictionary at the start of each run
    # global last_domain_access_time
    # last_domain_access_time.clear()

    # Prompt the user to enter one or more URLs
    target_urls_input = input("Enter URLs (comma-separated, e.g., https://notawebsite.org/, https://example.com/): ").strip()
    # Split the input string by commas and clean up whitespace, filtering out empty strings
    initial_urls = [url.strip() for url in target_urls_input.split(',') if url.strip()]

    # If no valid URLs were entered, exit the function
    if not initial_urls:
        print("No valid URLs entered.")
        return

    all_discovered_links = set() # Set to store all unique internal links found across all initial URLs
    for url in initial_urls:
        # Basic validation to ensure the URL starts with 'http' or 'https'
        if not url.startswith("http"):
            print(f"Invalid URL format for {url}. Skipping.")
            continue
        print(f"\nStarting crawl for initial URL: {url}")
        # Perform the BFS crawl for each initial URL
        discovered_links_for_url = crawl_website(url)
        # Add all links discovered from the current URL to the master set
        all_discovered_links.update(discovered_links_for_url)

    print(f"Found {len(all_discovered_links)} unique internal links across all initial URLs.")

    # Clear previous output before asking for archiving action to keep the console clean
    clear_output(wait=True)

    # Loop until a valid global archiving action is chosen by the user
    while True:
        global_choice = input(f"Choose global archiving action: 'A' (Archive All), 'N' (Archive Normally - respecting {SETTINGS['archiving_cooldown']*24}h rule), 'S' (Skip All): ").strip().lower()
        if global_choice in ['a', 'n', 's']:
            break # Exit loop if input is valid
        else:
            print("Invalid input. Please enter 'A', 'N', or 'S'.")

    # Print consolidated message for 'Archive All' or 'Skip All' actions
    if global_choice == 'a':
        print(f"Global choice: Archiving all {len(all_discovered_links)} discovered links.")
    elif global_choice == 's':
        print(f"Global choice: Skipping all {len(all_discovered_links)} discovered links.")

    results = [] # List to store the results of archiving attempts
    # Iterate through all discovered unique links (sorted for consistent output)
    for link in sorted(list(all_discovered_links)):
        # Process each link for archiving based on the global choice
        results.append(process_link_for_archiving(link, global_choice))

    # Clear output again before the final summary for cleanliness
    clear_output(wait=True)

    print("\n--- Archiving Summary ---")
    for result in results:
        print(result)

if __name__ == "__main__":
    main()
